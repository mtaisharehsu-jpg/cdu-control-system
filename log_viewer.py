#!/usr/bin/env python3
"""
CDUç³»çµ±æ—¥èªŒæŸ¥çœ‹å·¥å…·
ç”¨æ–¼æŸ¥çœ‹å’Œåˆ†ææ¯æ—¥æ—¥èªŒè¨˜éŒ„
"""

import os
import json
from datetime import datetime, timedelta
from pathlib import Path
import argparse

class LogViewer:
    """æ—¥èªŒæŸ¥çœ‹å™¨"""
    
    def __init__(self, log_base_dir="log"):
        self.log_base_dir = Path(log_base_dir)
    
    def list_available_dates(self):
        """åˆ—å‡ºå¯ç”¨çš„æ—¥èªŒæ—¥æœŸ"""
        if not self.log_base_dir.exists():
            print("æ—¥èªŒç›®éŒ„ä¸å­˜åœ¨")
            return []
        
        dates = []
        for item in self.log_base_dir.iterdir():
            if item.is_dir() and item.name.count('-') == 2:
                try:
                    datetime.strptime(item.name, "%Y-%m-%d")
                    dates.append(item.name)
                except ValueError:
                    continue
        
        return sorted(dates, reverse=True)
    
    def show_date_summary(self, date_str):
        """é¡¯ç¤ºæŒ‡å®šæ—¥æœŸçš„æ—¥èªŒæ‘˜è¦"""
        date_dir = self.log_base_dir / date_str
        if not date_dir.exists():
            print(f"æ—¥æœŸ {date_str} çš„æ—¥èªŒä¸å­˜åœ¨")
            return
        
        print(f"\nğŸ“… æ—¥èªŒæ‘˜è¦ - {date_str}")
        print("=" * 50)
        
        # æª¢æŸ¥æ‘˜è¦æ–‡ä»¶
        summary_file = date_dir / "daily_summary.json"
        if summary_file.exists():
            with open(summary_file, 'r', encoding='utf-8') as f:
                summary = json.load(f)
            print(f"ç”Ÿæˆæ™‚é–“: {summary.get('generated_at', 'Unknown')}")
        
        # åˆ—å‡ºæ—¥èªŒæ–‡ä»¶å’Œå¤§å°
        log_types = ["system", "sensors", "plc", "api", "errors"]
        for log_type in log_types:
            log_dir = date_dir / log_type
            if log_dir.exists():
                log_files = list(log_dir.glob("*.log"))
                total_size = sum(f.stat().st_size for f in log_files)
                file_count = len(log_files)
                print(f"ğŸ“ {log_type.upper()}: {file_count} å€‹æ–‡ä»¶, {total_size:,} bytes")
    
    def view_log_file(self, date_str, log_type, lines=50):
        """æŸ¥çœ‹æŒ‡å®šæ—¥èªŒæ–‡ä»¶çš„æœ€å¾Œå¹¾è¡Œ"""
        date_dir = self.log_base_dir / date_str / log_type
        if not date_dir.exists():
            print(f"æ—¥èªŒé¡å‹ {log_type} åœ¨ {date_str} ä¸å­˜åœ¨")
            return
        
        log_files = list(date_dir.glob("*.log"))
        if not log_files:
            print(f"åœ¨ {log_type} ç›®éŒ„ä¸­æ²’æœ‰æ‰¾åˆ°æ—¥èªŒæ–‡ä»¶")
            return
        
        # ä½¿ç”¨æœ€æ–°çš„æ—¥èªŒæ–‡ä»¶
        latest_log = max(log_files, key=lambda f: f.stat().st_mtime)
        
        print(f"\nğŸ“„ {log_type.upper()} æ—¥èªŒ - {date_str}")
        print(f"æ–‡ä»¶: {latest_log.name}")
        print("=" * 80)
        
        try:
            with open(latest_log, 'r', encoding='utf-8') as f:
                all_lines = f.readlines()
                
            # é¡¯ç¤ºæœ€å¾Œå¹¾è¡Œ
            display_lines = all_lines[-lines:] if len(all_lines) > lines else all_lines
            
            for line in display_lines:
                print(line.rstrip())
                
            print(f"\né¡¯ç¤ºäº†æœ€å¾Œ {len(display_lines)} è¡Œ (ç¸½å…± {len(all_lines)} è¡Œ)")
            
        except Exception as e:
            print(f"è®€å–æ—¥èªŒæ–‡ä»¶æ™‚å‡ºéŒ¯: {e}")
    
    def search_logs(self, date_str, keyword, log_type=None):
        """åœ¨æ—¥èªŒä¸­æœç´¢é—œéµå­—"""
        date_dir = self.log_base_dir / date_str
        if not date_dir.exists():
            print(f"æ—¥æœŸ {date_str} çš„æ—¥èªŒä¸å­˜åœ¨")
            return
        
        print(f"\nğŸ” æœç´¢é—œéµå­—: '{keyword}' åœ¨ {date_str}")
        print("=" * 60)
        
        search_dirs = [log_type] if log_type else ["system", "sensors", "plc", "api", "errors"]
        
        total_matches = 0
        for search_type in search_dirs:
            type_dir = date_dir / search_type
            if not type_dir.exists():
                continue
                
            log_files = list(type_dir.glob("*.log"))
            for log_file in log_files:
                try:
                    with open(log_file, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                    
                    matches = [(i+1, line.strip()) for i, line in enumerate(lines) 
                              if keyword.lower() in line.lower()]
                    
                    if matches:
                        print(f"\nğŸ“ {search_type.upper()} - {log_file.name}")
                        print("-" * 40)
                        for line_num, line in matches[:10]:  # æœ€å¤šé¡¯ç¤º10å€‹åŒ¹é…
                            print(f"{line_num:4d}: {line}")
                        
                        if len(matches) > 10:
                            print(f"... é‚„æœ‰ {len(matches) - 10} å€‹åŒ¹é…")
                        
                        total_matches += len(matches)
                        
                except Exception as e:
                    print(f"æœç´¢ {log_file} æ™‚å‡ºéŒ¯: {e}")
        
        print(f"\nç¸½å…±æ‰¾åˆ° {total_matches} å€‹åŒ¹é…")
    
    def generate_daily_report(self, date_str):
        """ç”Ÿæˆæ¯æ—¥å ±å‘Š"""
        date_dir = self.log_base_dir / date_str
        if not date_dir.exists():
            print(f"æ—¥æœŸ {date_str} çš„æ—¥èªŒä¸å­˜åœ¨")
            return
        
        print(f"\nğŸ“Š æ¯æ—¥å ±å‘Š - {date_str}")
        print("=" * 60)
        
        # çµ±è¨ˆå„é¡æ—¥èªŒçš„è¡Œæ•¸
        stats = {}
        for log_type in ["system", "sensors", "plc", "api", "errors"]:
            type_dir = date_dir / log_type
            if type_dir.exists():
                total_lines = 0
                for log_file in type_dir.glob("*.log"):
                    try:
                        with open(log_file, 'r', encoding='utf-8') as f:
                            total_lines += len(f.readlines())
                    except:
                        continue
                stats[log_type] = total_lines
        
        # é¡¯ç¤ºçµ±è¨ˆ
        for log_type, line_count in stats.items():
            print(f"ğŸ“ˆ {log_type.upper()}: {line_count:,} æ¢è¨˜éŒ„")
        
        # éŒ¯èª¤çµ±è¨ˆ
        error_dir = date_dir / "errors"
        if error_dir.exists():
            error_files = list(error_dir.glob("*.log"))
            if error_files:
                print(f"\nâš ï¸  éŒ¯èª¤åˆ†æ:")
                for error_file in error_files:
                    try:
                        with open(error_file, 'r', encoding='utf-8') as f:
                            error_lines = f.readlines()
                        print(f"   {error_file.name}: {len(error_lines)} å€‹éŒ¯èª¤")
                    except:
                        continue

def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description="CDUç³»çµ±æ—¥èªŒæŸ¥çœ‹å·¥å…·")
    parser.add_argument("--date", help="æŒ‡å®šæ—¥æœŸ (YYYY-MM-DD)")
    parser.add_argument("--list", action="store_true", help="åˆ—å‡ºå¯ç”¨æ—¥æœŸ")
    parser.add_argument("--type", choices=["system", "sensors", "plc", "api", "errors"], 
                       help="æ—¥èªŒé¡å‹")
    parser.add_argument("--lines", type=int, default=50, help="é¡¯ç¤ºè¡Œæ•¸")
    parser.add_argument("--search", help="æœç´¢é—œéµå­—")
    parser.add_argument("--report", action="store_true", help="ç”Ÿæˆæ¯æ—¥å ±å‘Š")
    
    args = parser.parse_args()
    
    viewer = LogViewer()
    
    if args.list:
        dates = viewer.list_available_dates()
        print("ğŸ“… å¯ç”¨çš„æ—¥èªŒæ—¥æœŸ:")
        for date in dates:
            print(f"  {date}")
        return
    
    # é»˜èªä½¿ç”¨ä»Šå¤©çš„æ—¥æœŸ
    target_date = args.date or datetime.now().strftime("%Y-%m-%d")
    
    if args.report:
        viewer.generate_daily_report(target_date)
    elif args.search:
        viewer.search_logs(target_date, args.search, args.type)
    elif args.type:
        viewer.view_log_file(target_date, args.type, args.lines)
    else:
        viewer.show_date_summary(target_date)

if __name__ == "__main__":
    main()
